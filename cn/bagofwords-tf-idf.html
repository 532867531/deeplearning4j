---
title: Bag of Words - TF-IDF
layout: cn-default
---
<p><h1>Bag of Words & TF-IDF</h1></p>
<p><a href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag of Words</a> (BoW) is an algorithm that counts how many times a word appears in a document. Those word counts allow us to compare documents and gauge their similarities for applications like search, document classification and topic modeling. BoW is a method for preparing text for input in a deep-learning net.</p>
<p>BoW lists words with their word counts per document. In the table where the words and documents effectively become vectors are stored, each row is a word, each column is a document and each cell is a word count. Each of the documents in the corpus is represented by columns of equal length. Those are wordcount vectors, an output stripped of context.</p>
<img class="img-responsive center-block" src="../img/wordcount-table.png" alt="deeplearning4j">
<p>Before they’re fed to the neural net, each vector of wordcounts is normalized such that all elements of the vector add up to one. Thus, the frequencies of each word is effectively converted to represent the probabilities of those words’ occurrence in the document. Probabilities that surpass certain levels will activate nodes in the net and influence the document’s classification.</p><br>
<p><h2>Term Frequency-Inverse Document Frequency (TF-IDF)</h2></p>
<p><a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">Term-frequency-inverse document frequency</a> (TF-IDF) is another way to judge the topic of an article by the words it contains. With TF-IDF, words are given weight – TF-IDF measures relevance, not frequency. That is, wordcounts are replaced with TF-IDF scores across the whole dataset.</p>
<p>First, TF-IDF measures the number of times that words appear in a given document (that&rsquo;s term frequency). But because words such as &ldquo;and&rdquo; or &ldquo;the&rdquo; appear frequently in all documents, those are systematically discounted. That&rsquo;s the inverse-document frequency part. The more documents a word appears in, the less valuable that word is as a signal. That&rsquo;s intended to leave only the frequent AND distinctive words as markers. Each word&rsquo;s TF-IDF relevance is a normalized data format that also adds up to one.</p><br>
<img class="img-responsive center-block" src="../img/tfidf.png" alt="deeplearning4j"><br>
<p>Those marker words are then fed to the neural net as features in order to determine the topic covered by the document that contains them.</p>
<p>Setting up a BoW looks something like this:</p>
<pre class="line-numbers"><code class="language-java">
public class BagOfWordsVectorizer extends BaseTextVectorizer {
      public BagOfWordsVectorizer(){}
      protected BagOfWordsVectorizer(VocabCache cache,
             TokenizerFactory tokenizerFactory,
             List&lt;String&gt; stopWords,
             int minWordFrequency,
             DocumentIterator docIter,
             SentenceIterator sentenceIterator,
             List
             &lt;String&gt; labels,
             InvertedIndex index,
             int batchSize,
             double sample,
             boolean stem,
             boolean cleanup) {
          super(cache, tokenizerFactory, stopWords, minWordFrequency, docIter, sentenceIterator,
              labels,index,batchSize,sample,stem,cleanup);
    }
</code></pre>
<p>While simple, TF-IDF is incredibly powerful, and contributes to such ubiquitous and useful tools as Google search.</p>
<p>BoW is different from <a href="word2vec.html">Word2vec</a>, which we&rsquo;ll cover next. The main difference is that Word2vec produces one vector per word, whereas BoW produces one number (a wordcount). Word2vec is great for digging into documents and identifying content and subsets of content. Its vectors represent each word&rsquo;s context, the ngrams of which it is a part. BoW is good for classifying documents as a whole.</p>    
<br>
<p><h2>Other Deeplearning4j Tutorials</h2></p>
<ul>
  <li><a href="word2vec">Word2vec</a></li>
  <li><a href="neuralnet-overview">Introduction to Neural Networks</a></li>
  <li><a href="restrictedboltzmannmachine">Restricted Boltzmann Machines</a></li>
  <li><a href="eigenvector">Eigenvectors, Covariance, PCA and Entropy</a></li>
  <li><a href="lstm">LSTMs and Recurrent Networks</a></li>
  <li><a href="linear-regression">Neural Networks and Regression</a></li>
  <li><a href="convolutionalnets">Convolutional Networks for Images</a></li>
</ul>
